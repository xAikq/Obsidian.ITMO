# Теория Вероятностей: Разбор Билетов

Это некоторые биллеты из общего банка биллетов экзамена по Теории Вероятностей. Ниже приведён подробный разбор каждого билета с доказательствами, теоремами и графиками функций там, где это необходимо.

---

## 1. Законы распределения непрерывных случайных величин

### Определение

Непрерывная случайная величина — это такая величина, принимающая значения из некоторого интервала на вещественной прямой. Её распределение описывается функцией плотности вероятности $( f_X(x))$, которая удовлетворяет условиям:

1. $( f_X(x) \geq 0 ) \space для \space всех \space ( x )$.
2. условие нормировки $$\int_{-\infty}^{+\infty} f_X(x) \, dx = 1.$$

### Основные законы распределения

#### 1. Равномерное распределение (Uniform Distribution)

- **Плотность вероятности:**
  $$f_{X}(x) = \begin{cases} \frac{1}{b - a}, & a \leq x \leq b, \\ 0, & \text{иначе}. \end{cases}$$
- **Математическое ожидание:**

$$M(x) = \frac{a+b}{2}$$

- **Дисперсия:**

$$D(x) = \frac{(b-a)^2}{12}$$

##### Примечание

Вычисление функции случайной величины равномерно распределённой на отрезке $(a;b):$

1. $x \leq a$
2. $a > x > b$
3. $x \geq b$

$$1) \space F_{\xi}(x) = \int_{-\infty}^{x} f_{\xi}(x) \space dx = 0$$

$$2) \space F_{\xi}(x) = \int_{-\infty}^{x} f_{\xi}(x) \space dx = \int_{-\infty}^{a} f_{\xi}(u) \space du + \int_{a}^{x}f_{\xi}(u) \space du = 0 + \frac{1}{b - a} * u\big|_{a}^{x} = \frac{x-a}{b-a}$$

$$3) \space F_{\xi}(x) = \int_{-\infty}^{x} f_{\xi}(x) \space dx = \int_{-\infty}^{a} f_{\xi}(u) \space du + \int_{a}^{b}f_{\xi}(u) \space du + \int_{b}^{x}f_{\xi}(u) \space du = 0 + \frac{b - a}{b - a} + 0 = 1$$


#### 2. Нормальное распределение (Normal Distribution)

- **Плотность вероятности:**
  $$f_X(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{ -\frac{(x - \mu)^2}{2\sigma^2} },$$
  где $(\mu)$ — математическое ожидание, $(\sigma^2)$ — дисперсия.

#### 3. Показательное распределение (Exponential Distribution)

- **Плотность вероятности:**
  $$f_X(x) =
  \begin{cases}
  \lambda e^{-\lambda x}, & x \geq 0, \\
  0, & \text{иначе},
  \end{cases}$$
  где $(\lambda > 0)$ — параметр распределения.

- **Математическое ожидание:** 

$$M(x) = \frac{1}{\lambda}$$

- **Дисперсия:**

$$D(x) = \frac{1}{\lambda^{2}}$$


### Свойства непрерывных распределений

- **Математическое ожидание:**
  $$M[X] = \int_{-\infty}^{+\infty} x f_X(x) \, dx.$$

- **Дисперсия:**
  $$D(X) = M[X^2] - (M[X])^2 = \int_{-\infty}^{+\infty} x^2 f_X(x) \, dx - (M[X])^2.$$

- **Функция распределения (CDF):**
  $$F_X(x) = P(X \leq x) = \int_{-\infty}^{x} f_X(t) \, dt.$$

---

## 2. Дисперсия нормально распределённых случайных величин

### Определение

Нормально распределённая случайная величина $( X )$ характеризуется двумя параметрами: математическим ожиданием $( \mu)$ и дисперсией $( \sigma^2 )$. Обозначается $( X \sim N(\mu, \sigma^2) )$.

### Формула дисперсии

Для нормально распределённой случайной величины дисперсия $( \sigma^2)$ является одним из параметров распределения и указывает на степень разброса значений вокруг среднего $(\mu)$.

### Доказательство

Вычислим дисперсию \( X \):
$$D(x) = M[X^2] - (M[X])^2.$$

Для нормального распределения:
$$M[X] = \mu,$$
$$M[X^2] = \int_{-\infty}^{+\infty} x^2 \frac{1}{\sigma \sqrt{2\pi}} e^{ -\frac{(x - \mu)^2}{2\sigma^2} } \ dx.$$

Путём стандартного интегрирования для нормального распределения получаем:
$$M[X^2] = \mu^2 + \sigma^2.$$

Таким образом:
$$D(X) = (\mu^2 + \sigma^2) - \mu^2 = \sigma^2.$$

### Свойства

- **Линеарность дисперсии:**
  Если $( a, b )$ — константы, то:
  $$D(aX + b) = a^2 D(X).$$

- **Сумма независимых нормально распределённых величин:**
  Если $( X \sim N(\mu_X, \sigma_X^2) )$ и $( Y \sim N(\mu_Y, \sigma_Y^2))$, и $( X )$ и $( Y )$ независимы, то $( X + Y \sim N(\mu_X + \mu_Y, \sigma_X^2 + \sigma_Y^2))$.

---

## 3. Пример, из которого следует, что некоррелированные случайные величины не обязательно являются независимыми

### Теорема

Если две случайные величины $( X )$ и $( Y )$ независимы, то они некоррелированные, то есть $( \text{COV}(X, Y) = 0)$. Однако обратное утверждение неверно: некоррелированные случайные величины не обязательно независимы.

### Пример

Рассмотрим случайные величины $( X )$ и $( Y )$, где $( X )$ равновероятно принимает значения $( -1 ) \ и \ ( +1 ), \ а ( Y = X^2 )$.

#### Определение

$$P(X = 1) = P(X = -1) = \frac{1}{2},$$
$$Y = X^2.$$

#### Проверка некоррелированности

1. **Математическое ожидание:**
   $$M[X] = (-1) \cdot \frac{1}{2} + 1 \cdot \frac{1}{2} = 0,$$
   $$M[Y] = 1 \cdot \frac{1}{2} + 1 \cdot \frac{1}{2} = 1.$$

2. **Ковариация:**
   $$M[XY] = M[X \cdot X^2] = M[X^3] = (-1)^3 \cdot \frac{1}{2} + 1^3 \cdot \frac{1}{2} = -\frac{1}{2} + \frac{1}{2} = 0,$$
   $$\text{COV}(X, Y) = M[XY] - M[X]M[Y] = 0 - 0 \cdot 1 = 0.$$

Таким образом, $( X )$ и $( Y )$ некоррелированные.

#### Проверка независимости

Для независимости должны выполняться:
$$P(Y = y | X = x) = P(Y = y).$$

Однако, $( Y )$ полностью определяется $( X )$ (так как $( Y = X^2 )$), что означает, что знание $( X )$ даёт полную информацию о $( Y )$. Например:
$$P(Y = 1 | X = 1) = 1,$$
$$P(Y = 1) = 1.$$

Чтобы продемонстрировать зависимость, рассмотрим другой пример.

#### Исправленный пример

Рассмотрим $( X )$, равновероятно принимающую $( -1 )$, $( 0 )$ и $( +1 )$ с вероятностями $( \frac{1}{4} )$, $( \frac{1}{2} )$ и $( \frac{1}{4} )$ соответственно, и $( Y = X^2 )$.

1. **Математическое ожидание:**
   $$M[X] = 0,$$
   $$M[Y] = \frac{1}{4} \cdot 1 + \frac{1}{2} \cdot 0 + \frac{1}{4} \cdot 1 = \frac{1}{2}.$$

2. **Ковариация:**
   $$M[XY] = (-1) \cdot 1 \cdot \frac{1}{4} + 0 \cdot 0 \cdot \frac{1}{2} + 1 \cdot 1 \cdot \frac{1}{4} = 0,$$
   $$\text{COV}(X, Y) = 0 - 0 \cdot \frac{1}{2} = 0.$$

Однако $( Y )$ зависит от $( X )$, так как знание $( X )$ определяет $( Y )$. Следовательно, $( X )$ и $( Y )$ некоррелированные, но зависимы.

### Вывод

Этот пример демонстрирует, что отсутствие корреляции $(( \text{COV}(X, Y) = 0))$ не гарантирует независимость случайных величин $( X )$ и $( Y )$.

---

## 4. Практическое применение неравенства Чебышёва

### Неравенство Чебышёва

Для любой случайной величины $( X )$ с конечным математическим ожиданием $( \mu = M[X] )$ и конечной дисперсией $( \sigma^2 = D(X) )$, выполняется неравенство:
$$P(|X - \mu| \geq k\sigma) \leq \frac{1}{k^2}, \quad \forall k > 0.$$

### Интерпретация

Неравенство Чебышёва устанавливает верхнюю границу вероятности того, что случайная величина отклоняется от своего среднего значения более чем на $( k )$ стандартных отклонений.

### Применение

1. **Оценка вероятности отклонения:**
   Используется для получения оценок в случаях, когда неизвестна форма распределения случайной величины.

   **Пример:**
   
   Пусть средний балл студента по экзамену равен $( 70 )$ баллов, а дисперсия $( 25 )$. Найдём верхнюю границу вероятности того, что балл отклонится от среднего более чем на $( 10 )$ баллов.
   
   Здесь $( \mu = 70 )$, $( \sigma = 5 )$, $( k = \frac{10}{5} = 2 )$.
   
   По неравенству Чебышёва:
   $$P(|X - 70| \geq 10) \leq \frac{1}{2^2} = \frac{1}{4} = 25\%.$$
   Это означает, что вероятность того, что студент получит балл ниже $( 60 )$ или выше $( 80 )$, не превышает $( 25\% )$.

2. **Страхование и управление рисками:**
   Помогает оценить риски в финансовых моделях, страховании и других областях, где требуется контроль за отклонениями от ожидаемых значений.

3. **Качество продукции:**
   В производстве используется для контроля качества, оценивая вероятность выхода продукции за пределы допустимых норм.

4. **Машинное обучение и статистика:**
   Используется для анализа стабильности оценок и определения доверительных интервалов.

---

## 5. Пример о плотности распределения суммы независимых случайных величин и его смысл

### Теорема о свёртке

Плотность вероятности суммы двух независимых случайных величин $( X )$ и $( Y )$ определяется как свёртка их плотностей:
$$f_{X+Y}(z) = \int_{-\infty}^{+\infty} f_X(x) f_Y(z - x) \, dx.$$

### Пример

Пусть $( X )$ и $( Y )$ — независимые случайные величины с одинаковым равномерным распределением на $([0, 1])$. Найдём плотность $( Z = X + Y )$.

#### Решение

1. **Определение плотностей:**
   
   $$f_X(x) = f_Y(x) = \begin{cases} 1, & 0 \leq x \leq 1, \\ 0, & \text{иначе}. \end{cases}$$

2. **Нахождение свёртки:**
   $$f_Z(z) = \int_{-\infty}^{+\infty} f_X(x) f_Y(z - x) \, dx = \int_{0}^{1} f_Y(z - x) \, dx.$$

   Поскольку $( f_Y(z - x) = 1 )$ только если $( 0 \leq z - x \leq 1 )$, это ограничивает пределы интегрирования.

   - Для $( 0 \leq z \leq 1 )$:
     $$x \in [0, z],$$
     $$f_Z(z) = \int_{0}^{z} 1 \, dx = z.$$

   - Для $( 1 < z \leq 2 )$:
     $$
     x \in [z - 1, 1],
     $$
     $$f_Z(z) = \int_{z - 1}^{1} 1 \, dx = 2 - z.$$

   - Для остальных \( z \):
     $$f_Z(z) = 0.$$

3. **Итоговая плотность:**
   $$f_Z(z) = \begin{cases} z, & 0 \leq z \leq 1, \\ 2 -z, & 1 < z \leq 2, \\ 0, & \text{иначе}. \end{cases}$$

### Смысл

Плотность суммы независимых случайных величин отражает то, насколько вероятно, что сумма примет определённое значение. В данном примере, значения суммы ближе к среднему (в $( z = 1 )$) наиболее вероятны, а крайние значения $(0 \ и \ 2)$ имеют наименьшую вероятность.

